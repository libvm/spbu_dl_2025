{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRDSx1wkqOrk"
      },
      "source": [
        "#Задание 1.\n",
        "\n",
        " Напишите функцию, которая моделирует один нейрон с сигмоидной активацией и реализует вычисление градиента для обновления весов и смещений нейрона. Функция должна принимать список векторов признаков, ассоциированные бинарные метки класса, начальные веса, начальное смещение, скорость обучения и количество эпох. Функция должна обновлять веса и смещение с помощью градиентного спуска (классической версии) на основе функции потерь NLL и возвращать обновленные веса, смещение и список значений NLL для каждой эпохи, округленное до четырех десятичных знаков. Проведите обучение на предоставленном наборе данных из задания 4 (для двух разных лет). Опционально сгенерируйте другие подходящие наборы данных. Опишите ваши результаты. Предоставленная функция будет также протестирована во время защиты ДЗ. Можно использовать только чистый torch (без использования autograd и torch.nn)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "La3nnRJErFhp"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Gnqn7kxMhNzS"
      },
      "outputs": [],
      "source": [
        "def activationf(z):\n",
        "  return 1/(1 + torch.exp(-z))\n",
        "\n",
        "def lossf(y, p):\n",
        "  return -(y * torch.log(p + 1e-8) + (1 - y) * torch.log(1 - p + 1e-8))\n",
        "\n",
        "def MLP(X, Y_vec, W_vec, b, lr, n_epochs):\n",
        "  X = torch.tensor(X, dtype=torch.float32)\n",
        "  Y_vec = torch.tensor(Y_vec, dtype=torch.float32)\n",
        "  W_vec = torch.tensor(W_vec, dtype=torch.float32)\n",
        "  b = torch.tensor(b, dtype=torch.float32)\n",
        "\n",
        "  losses = []\n",
        "  n_samples = len(X)\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    grad_w = torch.zeros_like(W_vec)\n",
        "    grad_b = torch.tensor(0.0)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "      X_vec = X[i]\n",
        "      y = Y_vec[i]\n",
        "\n",
        "      # Прямое распространение\n",
        "      z = torch.dot(X_vec, W_vec) + b\n",
        "      p = activationf(z)\n",
        "\n",
        "      # NLL лосс\n",
        "      iter_loss = lossf(y, p)\n",
        "      epoch_loss += iter_loss\n",
        "\n",
        "      # Обратное распространение\n",
        "      error = p - y\n",
        "      grad_w += X_vec * error\n",
        "      grad_b += error\n",
        "\n",
        "\n",
        "    grad_w /= n_samples\n",
        "    grad_b /= n_samples\n",
        "\n",
        "    # Обновляем веса в конце э похи после того, как мы прошлись\n",
        "    # по всем элементам выборки\n",
        "    W_vec -= lr * grad_w\n",
        "    b -= lr * grad_b\n",
        "\n",
        "    losses.append(epoch_loss / n_samples)\n",
        "\n",
        "  return W_vec, b, losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLQw699mmMNY"
      },
      "source": [
        "Проверка на примере"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "DpwXR2nWxpZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e0560c1-8794-44c8-ba40-eee3b2467f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([ 0.1070, -0.0847]), tensor(-0.0335), [tensor(0.8006), tensor(0.7631)])\n"
          ]
        }
      ],
      "source": [
        "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n",
        "labels = [1, 0, 0]\n",
        "initial_weights = [0.1, -0.2]\n",
        "initial_bias = 0.0\n",
        "learning_rate = 0.1\n",
        "epochs = 2\n",
        "\n",
        "result = MLP(features, labels,\n",
        "initial_weights, initial_bias,\n",
        "             learning_rate, epochs)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение на данных из задания 4"
      ],
      "metadata": {
        "id": "fLdYuBcZGDB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "rVFg0zxjEVmp"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных\n",
        "x = pd.read_csv('train_x.csv')\n",
        "y = pd.read_csv('train_y.csv')\n",
        "test_x = pd.read_csv('test_x.csv')\n",
        "\n",
        "x = x.rename(columns={x.columns[0]: 'id'})\n",
        "y = y.rename(columns={y.columns[0]: 'id'})\n",
        "\n",
        "data = pd.merge(x, y, on='id')\n",
        "test_ids = test_x['id'].values\n",
        "\n",
        "data = data[data['year'].isin([2003, 2004])]\n",
        "\n",
        "X_all = data.drop(['id', 'year'], axis=1).values.astype(np.float32)\n",
        "y_all = data['year'].values.astype(np.float32)\n",
        "\n",
        "y_all_binary = np.where(y_all == 2003, 0, 1).astype(np.float32)\n",
        "\n",
        "print(f\"Класс 0: {np.sum(y_all == 2003)} образцов\")\n",
        "print(f\"Класс 1: {np.sum(y_all == 2004)} образцов\")\n",
        "\n",
        "# Разделение на train и test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_all, y_all_binary, test_size=0.2, random_state=42, stratify=y_all_binary\n",
        ")"
      ],
      "metadata": {
        "id": "gXTOkHiFECi5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a124bd98-0ce1-47f4-b160-24a48316830e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Класс 0: 703 образцов\n",
            "Класс 1: 792 образцов\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры\n",
        "n_features = X_train.shape[1]\n",
        "W_initial = np.random.randn(n_features).astype(np.float32) * 0.01\n",
        "b_initial = 0.0\n",
        "learning_rate = 0.01\n",
        "epochs = 500"
      ],
      "metadata": {
        "id": "tNn0Zrh8ELc7"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_trained, b_trained, losses = MLP(X_train, y_train, W_initial, b_initial, learning_rate, epochs)\n",
        "print(f\"Final loss: {losses[-1]:.4f}\")"
      ],
      "metadata": {
        "id": "mNagmu1mEMEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ca5ad8-7f92-49ec-fccd-df201c995d6d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss: 8.6559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Предсказания на тренировочных данных\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "train_predictions = activationf(torch.matmul(X_train_tensor, W_trained) + b_trained)\n",
        "train_predicted_labels = (train_predictions > 0.5).float()\n",
        "\n",
        "# Точность на тренировочных данных\n",
        "train_accuracy = (train_predicted_labels == torch.tensor(y_train)).float().mean()\n",
        "print(f\"Train accuracy: {train_accuracy.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "C-NOEqUXEhaH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6c2af25-98e1-41b6-ed2d-b8832fd238a1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 47.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Предсказания на тестовых данных\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "test_predictions = activationf(torch.matmul(X_test_tensor, W_trained) + b_trained)\n",
        "test_predicted_labels = (test_predictions > 0.5).float()\n",
        "\n",
        "# Точность на тестовых данных\n",
        "test_accuracy = (test_predicted_labels == torch.tensor(y_test)).float().mean()\n",
        "print(f\"Test accuracy: {test_accuracy.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "_PeX5l5TEh54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97505bc2-682f-4666-a0e2-802270c6c17b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 47.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu8DzEawuR7P"
      },
      "source": [
        "#Задание 2.\n",
        "\n",
        "Реализуйте базовые функции autograd. Можете вдохновиться видео от Andrej Karpathy. Напишите класс, аналогичный предоставленному классу 'Element', который реализует основные операции autograd: сложение, умножение и активацию ReLU. Класс должен обрабатывать скалярные объекты и правильно вычислять градиенты для этих операций посредством автоматического дифференцирования. Плюсом будет набор предоставленных тестов, оценивающих правильность вычислений. Большим плюсом будет, если тесты будут написаны с помощью unittest. Можно использовать только чистый torch (без использования autograd и torch.nn). За каждую нереализованную операцию будет вычитаться 3 балла."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOIzcZilqCgl"
      },
      "source": [
        "Вдохновился видеороликом Andrey Karpathy, как и было указано в задании. Предусмотрел возможность работы напрямую со скалярами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6OQe2zPsuRVO"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "\tdef __init__(self, data, _children=(), _op=''):\n",
        "\t\tself.data = data\n",
        "\t\tself.grad = 0\n",
        "\t\tself._backward = lambda: None\n",
        "\t\tself._prev = set(_children)\n",
        "\t\tself._op = _op\n",
        "\n",
        "\tdef __repr__(self):\n",
        "\t\treturn f\" Element(data={self.data}, grad={self.grad})\"\n",
        "\n",
        "\tdef __add__(self, other):\n",
        "\t\tother = other if isinstance(other, Node) else Node(other)\n",
        "\t\tout = Node(self.data + other.data, (self, other), '+')\n",
        "\n",
        "\t\tdef _backward():\n",
        "\t\t\tself.grad += out.grad\n",
        "\t\t\tother.grad += out.grad\n",
        "\n",
        "\t\tout._backward = _backward\n",
        "\n",
        "\t\treturn out\n",
        "\n",
        "\tdef __mul__(self, other):\n",
        "\t\tother = other if isinstance(other, Node) else Node(other)\n",
        "\t\tout = Node(self.data * other.data, (self, other), '*')\n",
        "\n",
        "\t\tdef _backward():\n",
        "\t\t\tself.grad += other.data * out.grad\n",
        "\t\t\tother.grad += self.data * out.grad\n",
        "\n",
        "\t\tout._backward = _backward\n",
        "\n",
        "\t\treturn out\n",
        "\n",
        "\tdef relu(self):\n",
        "\t\tout = Node(0 if self.data < 0 else self.data, (self, ), 'relu')\n",
        "\n",
        "\t\tdef _backward():\n",
        "\t\t\tself.grad += (0 if self.data < 0 else 1) * out.grad\n",
        "\n",
        "\t\tout._backward = _backward\n",
        "\n",
        "\t\treturn out\n",
        "\n",
        "\tdef backward(self):\n",
        "\t\t# Топологическая сортировка для прохода backward-ом по графу\n",
        "\t\tvisited = set()\n",
        "\t\tsorted = []\n",
        "\n",
        "\t\tdef top_sort(v):\n",
        "\t\t\tif v not in visited:\n",
        "\t\t\t\tvisited.add(v)\n",
        "\t\t\t\tfor child in v._prev:\n",
        "\t\t\t\t\ttop_sort(child)\n",
        "\t\t\t\tsorted.append(v)\n",
        "\n",
        "\t\ttop_sort(self)\n",
        "\n",
        "\t\t# Градиент последней вершины равен 1\n",
        "\t\tself.grad = 1\n",
        "\t\t# Берем reversed, т.к. в backward идем с конца\n",
        "\t\tfor v in reversed(sorted):\n",
        "\t\t\tv._backward()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFZVCwdapozA"
      },
      "source": [
        "Юнит-тесты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XkB5a2WRpmXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e7909b1-b114-4d41-bd61-8a4516a8f129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_addition_gradient (__main__.TestNodeBackward.test_addition_gradient) ... ok\n",
            "test_expression_chain (__main__.TestNodeBackward.test_expression_chain) ... ok\n",
            "test_multiple_usage (__main__.TestNodeBackward.test_multiple_usage) ... ok\n",
            "test_multiplication_gradient (__main__.TestNodeBackward.test_multiplication_gradient) ... ok\n",
            "test_relu_negative (__main__.TestNodeBackward.test_relu_negative) ... ok\n",
            "test_relu_positive (__main__.TestNodeBackward.test_relu_positive) ... ok\n",
            "test_scalar_addition (__main__.TestNodeBackward.test_scalar_addition) ... ok\n",
            "test_scalar_multiplication (__main__.TestNodeBackward.test_scalar_multiplication) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 8 tests in 0.014s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=8 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import unittest\n",
        "from io import StringIO\n",
        "\n",
        "class TestNodeBackward(unittest.TestCase):\n",
        "    def test_addition_gradient(self):\n",
        "        # Проверка градиента для операции сложения\n",
        "        a = Node(2.0)\n",
        "        b = Node(3.0)\n",
        "        c = a + b\n",
        "        c.backward()\n",
        "\n",
        "        self.assertEqual(a.grad, 1.0)\n",
        "        self.assertEqual(b.grad, 1.0)\n",
        "        self.assertEqual(c.data, 5.0)\n",
        "\n",
        "    def test_multiplication_gradient(self):\n",
        "        # Проверка градиента для операции умножения\n",
        "        a = Node(2.0)\n",
        "        b = Node(3.0)\n",
        "        c = a * b\n",
        "        c.backward()\n",
        "\n",
        "        self.assertEqual(a.grad, 3.0)  # dc/da = b\n",
        "        self.assertEqual(b.grad, 2.0)  # dc/db = a\n",
        "        self.assertEqual(c.data, 6.0)\n",
        "\n",
        "    def test_relu_positive(self):\n",
        "        # Проверка ReLU для положительного числа\n",
        "        a = Node(2.0)\n",
        "        b = a.relu()\n",
        "        b.backward()\n",
        "\n",
        "        self.assertEqual(b.data, 2.0)\n",
        "        self.assertEqual(a.grad, 1.0)\n",
        "\n",
        "    def test_relu_negative(self):\n",
        "        # Проверка ReLU для отрицательного числа\n",
        "        a = Node(-2.0)\n",
        "        b = a.relu()\n",
        "        b.backward()\n",
        "\n",
        "        self.assertEqual(b.data, 0.0)\n",
        "        self.assertEqual(a.grad, 0.0)\n",
        "\n",
        "    def test_expression_chain(self):\n",
        "        # Проверка цепочки операций\n",
        "        a = Node(2.0)\n",
        "        b = Node(3.0)\n",
        "        c = a * b  # 6.0\n",
        "        d = c + Node(1.0)  # 7.0\n",
        "        e = d.relu()  # 7.0\n",
        "        e.backward()\n",
        "\n",
        "        self.assertEqual(e.data, 7.0)\n",
        "        self.assertEqual(a.grad, 3.0)  # de/da = de/dd * dd/dc * dc/da = 1 * 1 * b\n",
        "        self.assertEqual(b.grad, 2.0)  # de/db = de/dd * dd/dc * dc/db = 1 * 1 * a\n",
        "\n",
        "    def test_scalar_addition(self):\n",
        "        # Проверка сложения со скаляром\n",
        "        a = Node(2.0)\n",
        "        b = a + 3.0  # 5.0\n",
        "        b.backward()\n",
        "\n",
        "        self.assertEqual(b.data, 5.0)\n",
        "        self.assertEqual(a.grad, 1.0)\n",
        "\n",
        "    def test_scalar_multiplication(self):\n",
        "        # Проверка умножения на скаляр\n",
        "        a = Node(2.0)\n",
        "        b = a * 3.0  # 6.0\n",
        "        b.backward()\n",
        "\n",
        "        self.assertEqual(b.data, 6.0)\n",
        "        self.assertEqual(a.grad, 3.0)\n",
        "\n",
        "    def test_multiple_usage(self):\n",
        "        # Проверка множественного использования узла\n",
        "        a = Node(3.0)\n",
        "        b = a * a  # 9.0\n",
        "        b.backward()\n",
        "\n",
        "        self.assertEqual(b.data, 9.0)\n",
        "        self.assertEqual(a.grad, 6.0)  # db/da = 2*a\n",
        "\n",
        "# Создаем test suite и запускаем\n",
        "def run_tests():\n",
        "    suite = unittest.TestLoader().loadTestsFromTestCase(TestNodeBackward)\n",
        "    runner = unittest.TextTestRunner(verbosity=2)\n",
        "    return runner.run(suite)\n",
        "\n",
        "# Запускаем тесты в Jupyter\n",
        "run_tests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwHjA0YEu9fR"
      },
      "source": [
        "#Задание 3.\n",
        "\n",
        "Реализуйте один из оптимизаторов на выбор. Придумайте и напишите тесты для проверки выбранного оптимизатора. Проведите обучение нейрона из первого задания с использованием оптимизатора, а не ванильного градиентного спуска. Также опишите идею алгоритма (+1 балл). {*} Можете реализовать более 1 алгоритма. Каждый следующий даст 1 балл.\n",
        "\n",
        "Варианты:\n",
        "\n",
        "Momentum (3 балла)\n",
        "Nesterov (3 балла)\n",
        "Adagrad (4 балла)\n",
        "Adadelta (4 балла)\n",
        "RMSProp (5 баллов)\n",
        "Adam (5 баллов)\n",
        "Nadam (6 баллов)\n",
        "NAG (6 баллов)\n",
        "AdamW (6 баллов)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JSvowxgNLXU"
      },
      "source": [
        "Формула взята из хендбука Яндекса"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC2EKUxyNycM"
      },
      "source": [
        "\\begin{aligned}\n",
        "v_{k+1} &= \\beta_1 v_k + (1 - \\beta_1) \\nabla f(x_k) \\\\\n",
        "G_{k+1} &= \\beta_2 G_k + (1 - \\beta_2) (\\nabla f(x_k))^2 \\\\\n",
        "x_{k+1} &= x_k - \\left( \\frac{\\alpha}{\\sqrt{G_{k+1} + \\varepsilon}} v_{k+1} + \\lambda x_k \\right)\n",
        "\\end{aligned}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "M_aPgxY-vHfM"
      },
      "outputs": [],
      "source": [
        "class AdamW:\n",
        "  def __init__(self, params, beta_1, beta_2,\n",
        "               lr=0.001, eps=1e-8, weight_decay=0.01):\n",
        "    self.params = params\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "    self.lr = lr\n",
        "    self.eps = eps\n",
        "    self.weight_decay = weight_decay\n",
        "    self.steps = 0\n",
        "\n",
        "    self.v = [torch.zeros_like(param) for param in params]\n",
        "    self.G = [torch.zeros_like(param) for param in params]\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for param in self.params:\n",
        "      if param.grad is not None:\n",
        "        param.grad.zero_()\n",
        "\n",
        "  def step(self):\n",
        "    self.steps += 1\n",
        "    for i, param in enumerate(self.params):\n",
        "      if param.grad is None:\n",
        "        continue\n",
        "\n",
        "      grad = param.grad\n",
        "\n",
        "      self.v[i] = self.beta_1 * self.v[i] + (1 - self.beta_1) * param.grad\n",
        "\n",
        "      self.G[i] = self.beta_2 * self.G[i] + (1 - self.beta_2) * param.grad ** 2\n",
        "\n",
        "      v_hat = (1 / (1 - self.beta_1 ** self.steps)) * self.v[i]\n",
        "\n",
        "      G_hat = (1 / (1 - self.beta_2 ** self.steps)) * self.G[i]\n",
        "\n",
        "      param.data -= self.lr * v_hat / torch.sqrt(G_hat + self.eps) + self.weight_decay * param.data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "22yLm3rrLCc4"
      },
      "outputs": [],
      "source": [
        "def MLP_with_AdamW(X, Y_vec, W_vec, b, lr=0.01, n_epochs=1000, weight_decay=0.01):\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    Y_vec = torch.tensor(Y_vec, dtype=torch.float32)\n",
        "    W_vec = torch.tensor(W_vec, dtype=torch.float32, requires_grad=True)\n",
        "    b = torch.tensor(b, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "    # Инициализация оптимизатора\n",
        "    optimizer = AdamW([W_vec, b], 0.9, 0.9, lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    losses = []\n",
        "    n_samples = len(X)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        grad_w = torch.zeros_like(W_vec)\n",
        "        grad_b = torch.tensor(0.0)\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            X_vec = X[i]\n",
        "            y = Y_vec[i]\n",
        "\n",
        "            # Прямое распространение\n",
        "            z = torch.dot(X_vec, W_vec) + b\n",
        "            p = activationf(z)\n",
        "\n",
        "            # Вычисление потерь\n",
        "            iter_loss = lossf(y, p)\n",
        "            epoch_loss += iter_loss.item()\n",
        "\n",
        "            # Обратное распространение\n",
        "            error = p - y\n",
        "            grad_w += X_vec * error\n",
        "            grad_b += error\n",
        "\n",
        "        grad_w /= n_samples\n",
        "        grad_b /= n_samples\n",
        "\n",
        "        W_vec.grad = grad_w\n",
        "        b.grad = grad_b\n",
        "\n",
        "        # Обновление весов с AdamW\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        losses.append(epoch_loss / n_samples)\n",
        "\n",
        "    return W_vec.detach(), b.detach(), losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "bKcVyeBrNfvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2830393-31d6-4d84-de9f-a7f940defe1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([-0.7116,  0.9921]), tensor(-0.7098), [0.8006192048390707, 0.7914285858472189, 0.7825563351313273, 0.7739961942036947, 0.7657400369644165, 0.7577776710192362, 0.7500940958658854, 0.7426670789718628, 0.7354649305343628, 0.7284481922785441, 0.72157750527064, 0.7148226102193197, 0.7081662813822428, 0.7016016244888306, 0.6951272090276083, 0.6887446045875549, 0.6824559768040975, 0.676263431708018, 0.6701686580975851, 0.6641728679339091, 0.6582770148913065, 0.6524815758069357, 0.6467864712079366, 0.6411916216214498, 0.6356966892878214, 0.6303008596102396, 0.6250034769376119, 0.6198034683863322, 0.6146997312704722, 0.6096911032994589, 0.6047762632369995, 0.5999538401762644, 0.5952223539352417, 0.5905802647272745, 0.5860260725021362, 0.5815580089886984, 0.5771743257840475, 0.5728734234968821, 0.5686534345149994, 0.564512570699056, 0.5604488253593445, 0.5564603606859843, 0.5525452593962351, 0.548701673746109, 0.5449275573094686, 0.5412210822105408, 0.5375805199146271, 0.5340040822823843, 0.53049036860466, 0.5270377099514008, 0.5236448844273885, 0.5203106701374054, 0.517033855120341, 0.5138133664925894, 0.5106480518976847, 0.507536788781484, 0.5044784347216288, 0.5014716188112894, 0.4985150595506032, 0.4956076641877492, 0.49274810155232746, 0.4899352590243022, 0.48716820279757184, 0.48444597919782, 0.481767733891805, 0.47913287083307904, 0.4765406946341197, 0.47399059931437176, 0.47148191928863525, 0.4690139094988505, 0.46658602356910706, 0.46419745683670044, 0.4618474046389262, 0.45953498284022015, 0.4572596351305644, 0.455020268758138, 0.45281633734703064, 0.45064710577329, 0.4485119779904683, 0.446410208940506, 0.44434115290641785, 0.4423040449619293, 0.44029828906059265, 0.43832306067148846, 0.4363778630892436, 0.43446220954259235, 0.4325756182273229, 0.4307176073392232, 0.4288877348105113, 0.4270855784416199, 0.4253106117248535, 0.42356229821840924, 0.4218401064475377, 0.4201436738173167, 0.4184722950061162, 0.41682571172714233, 0.41520344217618305, 0.41360480586687726, 0.4120294253031413, 0.4104769478241603, 0.4089469661315282, 0.4074391523996989, 0.40595320363839466, 0.40448862314224243, 0.4030452271302541, 0.4016224443912506, 0.40021998186906177, 0.39883748193581897, 0.3974744925896327, 0.39613066613674164, 0.39480577905972797, 0.3934994290272395, 0.39221136768658954, 0.3909413516521454, 0.38968903323014575, 0.38845404982566833, 0.3872361828883489, 0.38603506485621136, 0.3848503877719243, 0.38368189831574756, 0.3825293630361557, 0.38139259318510693, 0.38027117649714154, 0.3791649838288625, 0.378073588013649, 0.376996989051501, 0.375934695204099, 0.3748865872621536, 0.3738524963458379, 0.3728320946296056, 0.3718251784642537, 0.3708315392335256, 0.3698509484529495, 0.3688831528027852, 0.36792802810668945, 0.3669852664073308, 0.36605475346247357, 0.36513622601826984, 0.36422955493132275, 0.36333446204662323, 0.36245080331961316, 0.3615783452987671, 0.3607169489065806, 0.3598664303620656, 0.3590266207853953, 0.35819730162620544, 0.357378289103508, 0.35656948387622833, 0.35577059785525006, 0.35498153666655224, 0.35420215129852295, 0.3534323175748189, 0.3526717871427536, 0.3519204258918762, 0.3511781344811122, 0.3504447390635808, 0.34972011546293896, 0.349004069964091, 0.3482965479294459, 0.3475973308086395, 0.3469063490629196, 0.34622346858183545, 0.3455485502878825, 0.3448815296093623, 0.34422214329242706, 0.3435705006122589, 0.34292636315027875, 0.34228963156541187, 0.3416602164506912, 0.3410380035638809, 0.34042292336622876, 0.33981484671433765, 0.33921367426713306, 0.3386193513870239, 0.33803173899650574, 0.33745085696379346, 0.3368764966726303, 0.33630864322185516, 0.33574723700682324, 0.33519210914770764, 0.33464327454566956, 0.33410067359606427, 0.3335641324520111, 0.3330336660146713, 0.3325091749429703, 0.33199062943458557, 0.33147789537906647, 0.33097095787525177, 0.3304697126150131, 0.3299741248289744, 0.32948410511016846, 0.32899952431519824, 0.32852039237817127, 0.3280465453863144, 0.3275780479113261, 0.32711481551329297, 0.32665670414765674, 0.3262036442756653, 0.32575563589731854, 0.3253125747044881, 0.3248744507630666, 0.32444117963314056, 0.3240126272042592, 0.3235888828833898, 0.32316982249418896, 0.32275527715682983, 0.32234537104765576, 0.32194003462791443, 0.3215390344460805, 0.3211425294478734, 0.3207503805557887, 0.32036251326402027, 0.3199789176384608, 0.3195994645357132, 0.3192242681980133, 0.31885312994321185, 0.3184860448042552, 0.3181229184071223, 0.317763810356458, 0.31740857164065045, 0.31705722709496814, 0.3167097568511963, 0.3163659820954005, 0.3160260021686554, 0.31568970779577893, 0.3153569946686427, 0.3150279223918915, 0.31470247606436413, 0.3143804967403412, 0.31406202415625256, 0.31374694406986237, 0.31343529125054675, 0.3131270209948222, 0.312822088599205, 0.31252043942610425, 0.3122220238049825, 0.31192682186762494, 0.3116347591082255, 0.311345840493838, 0.311060090859731, 0.31077738602956134, 0.3104977011680603, 0.31022097667058307, 0.30994724730650586, 0.30967647830645245, 0.3094085951646169, 0.3091435631116231, 0.3088813324769338, 0.3086219330628713, 0.30836532016595203, 0.30811144908269245, 0.30786027510960895, 0.30761179824670154, 0.30736585954825085, 0.3071226378281911, 0.30688194433848065, 0.30664388338724774, 0.3064083307981491, 0.30617526670296985, 0.3059446762005488, 0.30571649968624115, 0.3054907371600469, 0.30526741842428845, 0.3050464590390523, 0.3048277795314789, 0.3046114643414815, 0.3043974389632543, 0.304185688495636, 0.3039761433998744, 0.3037688086430232, 0.3035636047522227, 0.3033606559038162, 0.30315981308619183, 0.3029610912005107, 0.3027644455432892, 0.30256988604863483, 0.3023773680130641, 0.30218688646952313, 0.30199835697809857, 0.3018118490775426, 0.3016272683938344, 0.3014446645975113, 0.3012639433145523, 0.3010852038860321, 0.3009081929922104, 0.30073319872220355, 0.30055996278921765, 0.30038846532503766, 0.3002188404401143, 0.300050954023997, 0.2998848656813304, 0.2997204860051473, 0.2995578199625015, 0.2993968774875005, 0.29923755923906964, 0.2990799198547999, 0.29892392953236896, 0.2987696131070455, 0.29861684143543243, 0.29846567908922833, 0.2983161062002182, 0.29816804826259613, 0.2980215748151143, 0.29787658154964447, 0.2977331727743149, 0.2975911895434062, 0.29745065172513324, 0.2973116139570872, 0.29717402656873065, 0.29703785479068756, 0.2969031383593877, 0.2967697431643804, 0.29663776357968646, 0.29650719960530597, 0.29637792706489563, 0.29625000059604645, 0.29612340529759723, 0.29599812130133313, 0.29587413370609283, 0.2957514425118764, 0.29563000798225403, 0.29550980528195697, 0.29539092381795246, 0.29527317980925244, 0.295156662662824, 0.29504139721393585, 0.294927254319191, 0.2948143035173416, 0.2947025845448176, 0.2945919285217921, 0.2944824645916621, 0.29437410334746045, 0.2942668944597244, 0.29416080315907794, 0.2940557549397151, 0.29395176470279694, 0.2938489119211833, 0.2937470922867457, 0.29364631573359173, 0.29354657232761383, 0.2934478272994359, 0.29335009555021924, 0.29325341681639355, 0.2931576520204544, 0.2930629601081212, 0.2929692268371582, 0.2928764373064041, 0.29278462131818134, 0.2926936745643616, 0.2926037013530731, 0.292514647046725, 0.29242655138174695, 0.2923392951488495, 0.29225296278794605, 0.2921675145626068, 0.2920829157034556, 0.2919992556174596, 0.2919163803259532, 0.29183436930179596, 0.2917532076438268, 0.29167286058266956, 0.29159335295359295, 0.29151464998722076, 0.29143673181533813, 0.29135964810848236, 0.29128331442674, 0.29120772580305737, 0.29113303621610004, 0.29105902711550397, 0.29098573327064514, 0.2909132738908132, 0.2908415049314499, 0.2907704661289851, 0.29070014258225757, 0.2906305839618047, 0.290561705827713, 0.29049350321292877, 0.29042600591977435, 0.2903592487176259, 0.2902931372324626, 0.2902276913324992, 0.29016293088595074, 0.29009879132111865, 0.29003535707791644, 0.2899725486834844, 0.2899103065331777, 0.28984879950682324, 0.289787898461024, 0.2897275884946187, 0.28966788450876874, 0.28960883617401123, 0.2895503143469493, 0.2894924332698186, 0.2894351581732432, 0.28937841455141705, 0.28932230174541473, 0.2892667253812154, 0.2892117202281952, 0.28915730118751526, 0.28910335898399353, 0.28905002772808075, 0.28899726768334705, 0.288945014278094, 0.28889329731464386, 0.2888420621554057, 0.28879137833913165, 0.2887411564588547, 0.28869152069091797, 0.2886423369248708, 0.28859367966651917, 0.28854551911354065, 0.28849781056245166, 0.2884506285190582, 0.28840388854344684, 0.28835761547088623, 0.2883118788401286, 0.28826657434304553, 0.28822168707847595, 0.2881773312886556, 0.2881333331267039, 0.2880898416042328, 0.2880467623472214, 0.2880041152238846, 0.2879619548718135, 0.28792011241118115, 0.2878788312276204, 0.28783786793549854, 0.28779734174410504, 0.2877572625875473, 0.2877175509929657, 0.28767824669679004, 0.2876393496990204, 0.2876008003950119, 0.28756269812583923, 0.2875249634186427, 0.2874876062075297, 0.2874506364266078, 0.2874139994382858, 0.2873777349789937, 0.2873418927192688, 0.28730641802151996, 0.2872712214787801, 0.28723645210266113, 0.28720202048619586, 0.2871679464975993, 0.28713417053222656, 0.28710078199704486, 0.287067711353302, 0.28703494866689044, 0.28700257341067, 0.28697048127651215, 0.28693871200084686, 0.28690728545188904, 0.286876122156779, 0.2868453413248062, 0.28681481381257373, 0.28678464889526367, 0.2867548018693924, 0.2867252429326375, 0.2866959472497304, 0.28666697442531586, 0.2866382549206416, 0.28660983840624493, 0.28658172488212585, 0.28655385971069336, 0.28652627269426983, 0.2864990085363388, 0.28647201259930927, 0.2864452749490738, 0.2864188055197398, 0.2863926440477371, 0.2863667110602061, 0.2863410413265228, 0.28631563981374103, 0.2862904667854309, 0.2862655868132909, 0.28624093532562256, 0.28621654709180194, 0.28619236250718433, 0.2861684709787369, 0.28614477316538495, 0.2861213833093643, 0.2860981474320094, 0.2860751549402873, 0.2860524555047353, 0.2860299348831177, 0.2860076576471329, 0.285985603928566, 0.2859637836615245, 0.28594212234020233, 0.28592077394326526, 0.2858995944261551, 0.28587865829467773, 0.28585783143838245, 0.2858373175064723, 0.2858169873555501, 0.2857969005902608, 0.28577694793542224, 0.2857571989297867, 0.2857376386721929])\n"
          ]
        }
      ],
      "source": [
        "result = MLP_with_AdamW(features, labels, initial_weights, initial_bias,learning_rate, epochs)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W_trained, b_trained, losses = MLP_with_AdamW(X_train, y_train, W_initial, b_initial, learning_rate, epochs)\n",
        "print(f\"Final loss: {losses[-1]:.4f}\")"
      ],
      "metadata": {
        "id": "vxO8mG1UExT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00fd6ad2-d118-423c-94ae-20b66af90e9a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss: 1.4785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Предсказания на тренировочных данных\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "train_predictions = activationf(torch.matmul(X_train_tensor, W_trained) + b_trained)\n",
        "train_predicted_labels = (train_predictions > 0.5).float()\n",
        "\n",
        "# Точность на тренировочных данных\n",
        "train_accuracy = (train_predicted_labels == torch.tensor(y_train)).float().mean()\n",
        "print(f\"Train accuracy: {train_accuracy.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "m7lPcAyrE2pu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cd6991f-7c11-4765-d58c-f198416c18c3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 48.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Предсказания на тестовых данных (наша валидационная выборка)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "test_predictions = activationf(torch.matmul(X_test_tensor, W_trained) + b_trained)\n",
        "test_predicted_labels = (test_predictions > 0.5).float()\n",
        "\n",
        "# Точность на тестовых данных\n",
        "test_accuracy = (test_predicted_labels == torch.tensor(y_test)).float().mean()\n",
        "print(f\"Test accuracy: {test_accuracy.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "FB387oTWE22O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "489bf514-dddb-4fa3-e0db-b4e00bec0cf8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 47.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "_n6KPDEQE_Lj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvTucAgSam0e"
      },
      "source": [
        "#Задание 4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных\n",
        "train_x = pd.read_csv('train_x.csv')\n",
        "train_y = pd.read_csv('train_y.csv')\n",
        "test_x = pd.read_csv('test_x.csv')\n",
        "\n",
        "train_x = train_x.rename(columns={train_x.columns[0]: 'id'})\n",
        "train_y = train_y.rename(columns={train_y.columns[0]: 'id'})\n",
        "\n",
        "data = pd.merge(train_x, train_y, on='id')\n",
        "test_ids = test_x['id'].values\n",
        "\n",
        "X_all = data.drop(['id', 'year'], axis=1).values.astype(np.float32)\n",
        "y_all = data['year'].values.astype(np.float32)\n",
        "X_test = test_x.drop('id', axis=1).values.astype(np.float32)"
      ],
      "metadata": {
        "id": "snjMzfIQE9W3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выбрал ResNext, которую вы упомянули на практике. Пробовал TabresNet, разницы по MSE не было"
      ],
      "metadata": {
        "id": "TyISoaXHFOZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TabDataset(Dataset):\n",
        "    def __init__(self, features, targets=None):\n",
        "        self.features = features.astype(np.float32)\n",
        "        self.targets = targets.astype(np.float32) if targets is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.features[idx]).float()\n",
        "        if self.targets is not None:\n",
        "            y = torch.tensor(self.targets[idx]).float()\n",
        "            return x, y\n",
        "        return x\n",
        "\n",
        "class ResNeXtTabular(nn.Module):\n",
        "    def __init__(self, input_size, hidden_dim=512, cardinality=4, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.input_ln = nn.LayerNorm(input_size)\n",
        "        self.fc_in = nn.Linear(input_size, hidden_dim)\n",
        "        self.act = nn.Mish()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Mish(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Mish(),\n",
        "            nn.Dropout(dropout*0.8)\n",
        "        )\n",
        "\n",
        "        self.fc_mid = nn.Linear(hidden_dim, hidden_dim//2)\n",
        "        self.mid_ln = nn.LayerNorm(hidden_dim//2)\n",
        "        self.dropout_final = nn.Dropout(dropout*0.5)\n",
        "        self.fc_out = nn.Linear(hidden_dim//2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_ln(x)\n",
        "        x = self.fc_in(x)\n",
        "        x = self.act(x)\n",
        "        x = x + self.block1(x)\n",
        "        x = x + self.block2(x)\n",
        "        x = self.fc_mid(x)\n",
        "        x = self.act(self.mid_ln(x))\n",
        "        x = self.dropout_final(x)\n",
        "        x = self.fc_out(x)\n",
        "        return x.view(-1)"
      ],
      "metadata": {
        "id": "nJreJpCaFELV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAYJNg9CanEW"
      },
      "outputs": [],
      "source": [
        "# K-Fold кросс-валидация\n",
        "n_splits = 5  # Количество фолдов\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "test_preds_list = []  # Список для предсказаний с каждого фолда\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_all)):\n",
        "    print(f'Fold {fold + 1}/{n_splits}')\n",
        "\n",
        "    # Разделение данных\n",
        "    X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
        "    y_train, y_val = y_all[train_idx], y_all[val_idx]\n",
        "\n",
        "    # Масштабирование для каждого фолда\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # DataLoader\n",
        "    train_dataset = TabDataset(X_train, y_train)\n",
        "    val_dataset = TabDataset(X_val, y_val)\n",
        "    test_dataset = TabDataset(X_test_scaled)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
        "\n",
        "    # Инициализация модели для фолда\n",
        "    model = ResNeXtTabular(input_size=X_train.shape[1]).to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Обучение\n",
        "    for epoch in range(500):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Валидация\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb = xb.to(device)\n",
        "            preds = model(xb).cpu().numpy()\n",
        "            val_preds.append(preds)\n",
        "\n",
        "    val_preds = np.concatenate(val_preds).ravel()\n",
        "    val_mse = mean_squared_error(y_val, val_preds)\n",
        "    print(f\"Fold {fold + 1} Validation MSE: {val_mse:.6f}\")\n",
        "\n",
        "    # Предсказание на тесте\n",
        "    test_preds = []\n",
        "    with torch.no_grad():\n",
        "        for xb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            preds = model(xb).cpu().numpy()\n",
        "            test_preds.append(preds)\n",
        "\n",
        "    test_preds_list.append(np.concatenate(test_preds).ravel())\n",
        "\n",
        "# Усреднение предсказаний по фолдам\n",
        "test_preds_avg = np.mean(test_preds_list, axis=0)\n",
        "final_test_preds = np.rint(test_preds_avg).astype(int)\n",
        "\n",
        "# Создание submission файла\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_ids,\n",
        "    'year': final_test_preds\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission saved to submission.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}