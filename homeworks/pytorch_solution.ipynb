{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRDSx1wkqOrk"
      },
      "source": [
        "#Задание 1.\n",
        "\n",
        " Напишите функцию, которая моделирует один нейрон с сигмоидной активацией и реализует вычисление градиента для обновления весов и смещений нейрона. Функция должна принимать список векторов признаков, ассоциированные бинарные метки класса, начальные веса, начальное смещение, скорость обучения и количество эпох. Функция должна обновлять веса и смещение с помощью градиентного спуска (классической версии) на основе функции потерь NLL и возвращать обновленные веса, смещение и список значений NLL для каждой эпохи, округленное до четырех десятичных знаков. Проведите обучение на предоставленном наборе данных из задания 4 (для двух разных лет). Опционально сгенерируйте другие подходящие наборы данных. Опишите ваши результаты. Предоставленная функция будет также протестирована во время защиты ДЗ. Можно использовать только чистый torch (без использования autograd и torch.nn)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "La3nnRJErFhp"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnqn7kxMhNzS"
      },
      "outputs": [],
      "source": [
        "def activationf(z):\n",
        "  return 1/(1 + torch.exp(-z))\n",
        "\n",
        "def lossf(y, p):\n",
        "  return -(y * torch.log(p + 1e-8) + (1 - y) * torch.log(1 - p + 1e-8)) # + eps - защита от деления на 0\n",
        "\n",
        "def MLP(X, Y_vec, W_vec, b, lr, n_epochs):\n",
        "  X = torch.tensor(X, dtype=torch.float32)\n",
        "  Y_vec = torch.tensor(Y_vec, dtype=torch.float32)\n",
        "  W_vec = torch.tensor(W_vec, dtype=torch.float32)\n",
        "  b = torch.tensor(b, dtype=torch.float32)\n",
        "\n",
        "  losses = []\n",
        "  n_samples = len(X)\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    grad_w = torch.zeros_like(W_vec)\n",
        "    grad_b = torch.tensor(0.0)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "      X_vec = X[i]\n",
        "      y = Y_vec[i]\n",
        "\n",
        "      # Прямое распространение\n",
        "      z = torch.dot(X_vec, W_vec) + b\n",
        "      p = activationf(z)\n",
        "\n",
        "      # NLL лосс\n",
        "      iter_loss = lossf(y, p)\n",
        "      epoch_loss += iter_loss\n",
        "\n",
        "      # Обратное распространение\n",
        "      error = p - y # dl/dp * dp/dz\n",
        "      grad_w += X_vec * error # dl/dp * dp/dz * dz/dw\n",
        "      grad_b += error # dl/dp * dp/dz * dz/db(1)\n",
        "\n",
        "\n",
        "    grad_w /= n_samples\n",
        "    grad_b /= n_samples\n",
        "\n",
        "    # Обновляем веса в конце э похи после того, как мы прошлись\n",
        "    # по всем элементам выборки\n",
        "    W_vec -= lr * grad_w\n",
        "    b -= lr * grad_b\n",
        "\n",
        "    losses.append(epoch_loss / n_samples)\n",
        "\n",
        "  return W_vec, b, losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLQw699mmMNY"
      },
      "source": [
        "Проверка на примере"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpwXR2nWxpZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7d7432-4c3d-4c8f-922b-c689ff5f66f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([ 0.1070, -0.0847]), tensor(-0.0335), [tensor(0.8006), tensor(0.7631)])\n"
          ]
        }
      ],
      "source": [
        "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n",
        "labels = [1, 0, 0]\n",
        "initial_weights = [0.1, -0.2]\n",
        "initial_bias = 0.0\n",
        "learning_rate = 0.1\n",
        "epochs = 2\n",
        "\n",
        "result = MLP(features, labels,\n",
        "initial_weights, initial_bias,\n",
        "             learning_rate, epochs)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение на данных из задания 4"
      ],
      "metadata": {
        "id": "fLdYuBcZGDB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "rVFg0zxjEVmp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для бинарной классификации выбрал года 2003 и 2004, обучал только на них."
      ],
      "metadata": {
        "id": "Ti1UDNkvazaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных\n",
        "x = pd.read_csv('train_x.csv')\n",
        "y = pd.read_csv('train_y.csv')\n",
        "test_x = pd.read_csv('test_x.csv')\n",
        "\n",
        "x = x.rename(columns={x.columns[0]: 'id'})\n",
        "y = y.rename(columns={y.columns[0]: 'id'})\n",
        "\n",
        "data = pd.merge(x, y, on='id')\n",
        "test_ids = test_x['id'].values\n",
        "\n",
        "data = data[data['year'].isin([2003, 2004])]\n",
        "\n",
        "X_all = data.drop(['id', 'year'], axis=1).values.astype(np.float32)\n",
        "y_all = data['year'].values.astype(np.float32)\n",
        "\n",
        "y_all_binary = np.where(y_all == 2003, 0, 1).astype(np.float32)\n",
        "\n",
        "print(f\"Класс 0: {np.sum(y_all == 2003)} образцов\")\n",
        "print(f\"Класс 1: {np.sum(y_all == 2004)} образцов\")\n",
        "\n",
        "# Разделение на train и test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_all, y_all_binary, test_size=0.2, random_state=42, stratify=y_all_binary\n",
        ")"
      ],
      "metadata": {
        "id": "gXTOkHiFECi5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e43e2646-4eb1-42fc-9eb2-25d59bc9de5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Класс 0: 703 образцов\n",
            "Класс 1: 792 образцов\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Имеем относительно сбалансированные классы. Вне зависимости от того, какие параметры мы выбираем, аккураси не поднимается выше ~50%. Одного слоя с одним нейроном явно мало."
      ],
      "metadata": {
        "id": "R7TI_PxAXaQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры\n",
        "n_features = X_train.shape[1]\n",
        "W_initial = np.random.randn(n_features).astype(np.float32) * 0.01\n",
        "b_initial = 0.0\n",
        "learning_rate = 0.01\n",
        "epochs = 500"
      ],
      "metadata": {
        "id": "tNn0Zrh8ELc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_trained, b_trained, losses = MLP(X_train, y_train, W_initial, b_initial, learning_rate, epochs)\n",
        "print(f\"Final loss: {losses[-1]:.4f}\")"
      ],
      "metadata": {
        "id": "mNagmu1mEMEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fd953f1-1af0-4706-826f-633c6a3a8a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss: 9.7495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Предсказания на тренировочных данных\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "train_predictions = activationf(torch.matmul(X_train_tensor, W_trained) + b_trained)\n",
        "train_predicted_labels = (train_predictions > 0.5).float()\n",
        "\n",
        "# Точность на тренировочных данных\n",
        "train_accuracy = (train_predicted_labels == torch.tensor(y_train)).float().mean()\n",
        "print(f\"Train accuracy: {train_accuracy.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "C-NOEqUXEhaH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa36e252-8316-4f16-b15e-076053174f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 53.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Предсказания на тестовых данных\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "test_predictions = activationf(torch.matmul(X_test_tensor, W_trained) + b_trained)\n",
        "test_predicted_labels = (test_predictions > 0.5).float()\n",
        "\n",
        "# Точность на тестовых данных\n",
        "test_accuracy = (test_predicted_labels == torch.tensor(y_test)).float().mean()\n",
        "print(f\"Test accuracy: {test_accuracy.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "_PeX5l5TEh54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561b3707-eb62-4e4d-d274-8e1844172b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 52.84%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu8DzEawuR7P"
      },
      "source": [
        "#Задание 2.\n",
        "\n",
        "Реализуйте базовые функции autograd. Можете вдохновиться видео от Andrej Karpathy. Напишите класс, аналогичный предоставленному классу 'Element', который реализует основные операции autograd: сложение, умножение и активацию ReLU. Класс должен обрабатывать скалярные объекты и правильно вычислять градиенты для этих операций посредством автоматического дифференцирования. Плюсом будет набор предоставленных тестов, оценивающих правильность вычислений. Большим плюсом будет, если тесты будут написаны с помощью unittest. Можно использовать только чистый torch (без использования autograd и torch.nn). За каждую нереализованную операцию будет вычитаться 3 балла."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOIzcZilqCgl"
      },
      "source": [
        "Вдохновился видеороликом Andrey Karpathy: The spelled-out intro to neural networks and backpropagation: building micrograd, как и было указано в задании. Предусмотрел возможность работы напрямую со скалярами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OQe2zPsuRVO"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "\tdef __init__(self, data, _children=(), _op=''):\n",
        "\t\tself.data = data\n",
        "\t\tself.grad = 0\n",
        "\t\tself._backward = lambda: None\n",
        "\t\tself._prev = set(_children)\n",
        "\t\tself._op = _op\n",
        "\n",
        "\tdef __repr__(self):\n",
        "\t\treturn f\" Element(data={self.data}, grad={self.grad})\"\n",
        "\n",
        "\tdef __add__(self, other):\n",
        "\t\tother = other if isinstance(other, Node) else Node(other)\n",
        "\t\tout = Node(self.data + other.data, (self, other), '+')\n",
        "\n",
        "\t\tdef _backward():\n",
        "\t\t\tself.grad += out.grad\n",
        "\t\t\tother.grad += out.grad\n",
        "\n",
        "\t\tout._backward = _backward\n",
        "\n",
        "\t\treturn out\n",
        "\n",
        "\tdef __mul__(self, other):\n",
        "\t\tother = other if isinstance(other, Node) else Node(other)\n",
        "\t\tout = Node(self.data * other.data, (self, other), '*')\n",
        "\n",
        "\t\tdef _backward():\n",
        "\t\t\tself.grad += other.data * out.grad\n",
        "\t\t\tother.grad += self.data * out.grad\n",
        "\n",
        "\t\tout._backward = _backward\n",
        "\n",
        "\t\treturn out\n",
        "\n",
        "\tdef relu(self):\n",
        "\t\tout = Node(0 if self.data < 0 else self.data, (self, ), 'relu')\n",
        "\n",
        "\t\tdef _backward():\n",
        "\t\t\tself.grad += (0 if self.data < 0 else 1) * out.grad\n",
        "\n",
        "\t\tout._backward = _backward\n",
        "\n",
        "\t\treturn out\n",
        "\n",
        "\tdef backward(self):\n",
        "\t\t# Топологическая сортировка для прохода backward-ом по графу\n",
        "\t\tvisited = set()\n",
        "\t\tsorted = []\n",
        "\n",
        "\t\tdef top_sort(v):\n",
        "\t\t\tif v not in visited:\n",
        "\t\t\t\tvisited.add(v)\n",
        "\t\t\t\tfor child in v._prev:\n",
        "\t\t\t\t\ttop_sort(child)\n",
        "\t\t\t\tsorted.append(v)\n",
        "\n",
        "\t\ttop_sort(self)\n",
        "\n",
        "\t\t# Градиент последней вершины равен 1\n",
        "\t\tself.grad = 1\n",
        "\t\t# Берем reversed, т.к. в backward идем с конца\n",
        "\t\tfor v in reversed(sorted):\n",
        "\t\t\tv._backward()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFZVCwdapozA"
      },
      "source": [
        "Реализовал юнит-тесты с проверкой основных edge-кейсов по типу: операции с числами без обертки Node, вычисление градиента для нод, которые используются несколько раз"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkB5a2WRpmXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e067c768-c3b5-401e-c501-dd77b0e57fa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_addition_gradient (__main__.TestNodeBackward.test_addition_gradient) ... ok\n",
            "test_expression_chain (__main__.TestNodeBackward.test_expression_chain) ... ok\n",
            "test_multiple_usage (__main__.TestNodeBackward.test_multiple_usage) ... ok\n",
            "test_multiplication_gradient (__main__.TestNodeBackward.test_multiplication_gradient) ... ok\n",
            "test_relu_negative (__main__.TestNodeBackward.test_relu_negative) ... ok\n",
            "test_relu_positive (__main__.TestNodeBackward.test_relu_positive) ... ok\n",
            "test_scalar_addition (__main__.TestNodeBackward.test_scalar_addition) ... ok\n",
            "test_scalar_multiplication (__main__.TestNodeBackward.test_scalar_multiplication) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 8 tests in 0.018s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=8 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "import unittest\n",
        "from io import StringIO\n",
        "\n",
        "class TestNodeBackward(unittest.TestCase):\n",
        "    def test_addition_gradient(self):\n",
        "        # Проверка градиента для операции сложения\n",
        "        a = Node(2.0)\n",
        "        b = Node(3.0)\n",
        "        c = a + b\n",
        "        c.backward()\n",
        "\n",
        "        self.assertEqual(a.grad, 1.0)\n",
        "        self.assertEqual(b.grad, 1.0)\n",
        "        self.assertEqual(c.data, 5.0)\n",
        "\n",
        "    def test_multiplication_gradient(self):\n",
        "        # Проверка градиента для операции умножения\n",
        "        a = Node(2.0)\n",
        "        b = Node(3.0)\n",
        "        c = a * b\n",
        "        c.backward()\n",
        "\n",
        "        self.assertEqual(a.grad, 3.0)  # dc/da = b\n",
        "        self.assertEqual(b.grad, 2.0)  # dc/db = a\n",
        "        self.assertEqual(c.data, 6.0)\n",
        "\n",
        "    def test_relu_positive(self):\n",
        "        # Проверка ReLU для положительного числа\n",
        "        a = Node(2.0)\n",
        "        b = a.relu()\n",
        "        b.backward()\n",
        "\n",
        "        self.assertEqual(b.data, 2.0)\n",
        "        self.assertEqual(a.grad, 1.0)\n",
        "\n",
        "    def test_relu_negative(self):\n",
        "        # Проверка ReLU для отрицательного числа\n",
        "        a = Node(-2.0)\n",
        "        b = a.relu()\n",
        "        b.backward()\n",
        "\n",
        "        self.assertEqual(b.data, 0.0)\n",
        "        self.assertEqual(a.grad, 0.0)\n",
        "\n",
        "    def test_expression_chain(self):\n",
        "        # Проверка цепочки операций\n",
        "        a = Node(2.0)\n",
        "        b = Node(3.0)\n",
        "        c = a * b  # 6.0\n",
        "        d = c + Node(1.0)  # 7.0\n",
        "        e = d.relu()  # 7.0\n",
        "        e.backward()\n",
        "\n",
        "        self.assertEqual(e.data, 7.0)\n",
        "        self.assertEqual(a.grad, 3.0)  # de/da = de/dd * dd/dc * dc/da = 1 * 1 * b\n",
        "        self.assertEqual(b.grad, 2.0)  # de/db = de/dd * dd/dc * dc/db = 1 * 1 * a\n",
        "\n",
        "    def test_scalar_addition(self):\n",
        "        # Проверка сложения со скаляром\n",
        "        a = Node(2.0)\n",
        "        b = a + 3.0  # 5.0\n",
        "        b.backward()\n",
        "\n",
        "        self.assertEqual(b.data, 5.0)\n",
        "        self.assertEqual(a.grad, 1.0)\n",
        "\n",
        "    def test_scalar_multiplication(self):\n",
        "        # Проверка умножения на скаляр\n",
        "        a = Node(2.0)\n",
        "        b = a * 3.0  # 6.0\n",
        "        b.backward()\n",
        "\n",
        "        self.assertEqual(b.data, 6.0)\n",
        "        self.assertEqual(a.grad, 3.0)\n",
        "\n",
        "    def test_multiple_usage(self):\n",
        "        # Проверка множественного использования узла\n",
        "        a = Node(3.0)\n",
        "        b = a * a  # 9.0\n",
        "        b.backward()\n",
        "\n",
        "        self.assertEqual(b.data, 9.0)\n",
        "        self.assertEqual(a.grad, 6.0)  # db/da = 2*a\n",
        "\n",
        "# Создаем test suite и запускаем\n",
        "def run_tests():\n",
        "    suite = unittest.TestLoader().loadTestsFromTestCase(TestNodeBackward)\n",
        "    runner = unittest.TextTestRunner(verbosity=2)\n",
        "    return runner.run(suite)\n",
        "\n",
        "# Запускаем тесты в Jupyter\n",
        "run_tests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwHjA0YEu9fR"
      },
      "source": [
        "#Задание 3.\n",
        "\n",
        "Реализуйте один из оптимизаторов на выбор. Придумайте и напишите тесты для проверки выбранного оптимизатора. Проведите обучение нейрона из первого задания с использованием оптимизатора, а не ванильного градиентного спуска. Также опишите идею алгоритма (+1 балл). {*} Можете реализовать более 1 алгоритма. Каждый следующий даст 1 балл.\n",
        "\n",
        "Варианты:\n",
        "\n",
        "Momentum (3 балла)\n",
        "Nesterov (3 балла)\n",
        "Adagrad (4 балла)\n",
        "Adadelta (4 балла)\n",
        "RMSProp (5 баллов)\n",
        "Adam (5 баллов)\n",
        "Nadam (6 баллов)\n",
        "NAG (6 баллов)\n",
        "AdamW (6 баллов)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JSvowxgNLXU"
      },
      "source": [
        "Выбран оптимизатор AdamW. Формула снизу взята из хендбука Яндекса по машинному обучению"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC2EKUxyNycM"
      },
      "source": [
        "\\begin{aligned}\n",
        "v_{k+1} &= \\beta_1 v_k + (1 - \\beta_1) \\nabla f(x_k) \\\\\n",
        "G_{k+1} &= \\beta_2 G_k + (1 - \\beta_2) (\\nabla f(x_k))^2 \\\\\n",
        "x_{k+1} &= x_k - \\left( \\frac{\\alpha}{\\sqrt{G_{k+1} + \\varepsilon}} v_{k+1} + \\lambda x_k \\right)\n",
        "\\end{aligned}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_aPgxY-vHfM"
      },
      "outputs": [],
      "source": [
        "class AdamW:\n",
        "  def __init__(self, params, beta_1, beta_2,\n",
        "               lr=0.001, eps=1e-8, weight_decay=0.01):\n",
        "    self.params = params\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "    self.lr = lr\n",
        "    self.eps = eps\n",
        "    self.weight_decay = weight_decay\n",
        "    self.steps = 0\n",
        "\n",
        "    self.v = [torch.zeros_like(param) for param in params]\n",
        "    self.G = [torch.zeros_like(param) for param in params]\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for param in self.params:\n",
        "      if param.grad is not None:\n",
        "        param.grad.zero_()\n",
        "\n",
        "  def step(self):\n",
        "    self.steps += 1\n",
        "    for i, param in enumerate(self.params):\n",
        "      if param.grad is None:\n",
        "        continue\n",
        "\n",
        "      grad = param.grad\n",
        "\n",
        "      self.v[i] = self.beta_1 * self.v[i] + (1 - self.beta_1) * param.grad\n",
        "\n",
        "      self.G[i] = self.beta_2 * self.G[i] + (1 - self.beta_2) * param.grad ** 2\n",
        "\n",
        "      v_hat = (1 / (1 - self.beta_1 ** self.steps)) * self.v[i]\n",
        "\n",
        "      G_hat = (1 / (1 - self.beta_2 ** self.steps)) * self.G[i]\n",
        "\n",
        "      param.data -= self.lr * v_hat / torch.sqrt(G_hat + self.eps) + self.weight_decay * param.data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22yLm3rrLCc4"
      },
      "outputs": [],
      "source": [
        "def MLP_with_AdamW(X, Y_vec, W_vec, b, lr=0.01, n_epochs=1000, weight_decay=0.01):\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    Y_vec = torch.tensor(Y_vec, dtype=torch.float32)\n",
        "    W_vec = torch.tensor(W_vec, dtype=torch.float32, requires_grad=True)\n",
        "    b = torch.tensor(b, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "    # Инициализация оптимизатора\n",
        "    optimizer = AdamW([W_vec, b], 0.9, 0.9, lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    losses = []\n",
        "    n_samples = len(X)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        grad_w = torch.zeros_like(W_vec)\n",
        "        grad_b = torch.tensor(0.0)\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            X_vec = X[i]\n",
        "            y = Y_vec[i]\n",
        "\n",
        "            # Прямое распространение\n",
        "            z = torch.dot(X_vec, W_vec) + b\n",
        "            p = activationf(z)\n",
        "\n",
        "            # Вычисление потерь\n",
        "            iter_loss = lossf(y, p)\n",
        "            epoch_loss += iter_loss.item()\n",
        "\n",
        "            # Обратное распространение\n",
        "            error = p - y\n",
        "            grad_w += X_vec * error\n",
        "            grad_b += error\n",
        "\n",
        "        grad_w /= n_samples\n",
        "        grad_b /= n_samples\n",
        "\n",
        "        W_vec.grad = grad_w\n",
        "        b.grad = grad_b\n",
        "\n",
        "        # Обновление весов с AdamW\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        losses.append(epoch_loss / n_samples)\n",
        "\n",
        "    return W_vec.detach(), b.detach(), losses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лосс стал меньше, повысилось аккураси на трейне, но если сделать несколько прогонов, то почти всегда при этом падает аккураси на тесте"
      ],
      "metadata": {
        "id": "zkvkgIwVbWyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_trained, b_trained, losses = MLP_with_AdamW(X_train, y_train, W_initial, b_initial, learning_rate, epochs)\n",
        "print(f\"Final loss: {losses[-1]:.4f}\")"
      ],
      "metadata": {
        "id": "vxO8mG1UExT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c72e7a0c-400e-4e47-d3cd-92478c0a511c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss: 2.6669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Предсказания на тренировочных данных\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "train_predictions = activationf(torch.matmul(X_train_tensor, W_trained) + b_trained)\n",
        "train_predicted_labels = (train_predictions > 0.5).float()\n",
        "\n",
        "# Точность на тренировочных данных\n",
        "train_accuracy = (train_predicted_labels == torch.tensor(y_train)).float().mean()\n",
        "print(f\"Train accuracy: {train_accuracy.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "m7lPcAyrE2pu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c20d25-7078-41c8-a71d-e2e8c392547f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 55.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Предсказания на тестовых данных (наша валидационная выборка)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "test_predictions = activationf(torch.matmul(X_test_tensor, W_trained) + b_trained)\n",
        "test_predicted_labels = (test_predictions > 0.5).float()\n",
        "\n",
        "# Точность на тестовых данных\n",
        "test_accuracy = (test_predicted_labels == torch.tensor(y_test)).float().mean()\n",
        "print(f\"Test accuracy: {test_accuracy.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "FB387oTWE22O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40408a4a-79d2-4350-d297-e3cdadb45caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 52.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "_n6KPDEQE_Lj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvTucAgSam0e"
      },
      "source": [
        "#Задание 4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных\n",
        "train_x = pd.read_csv('train_x.csv')\n",
        "train_y = pd.read_csv('train_y.csv')\n",
        "test_x = pd.read_csv('test_x.csv')\n",
        "\n",
        "train_x = train_x.rename(columns={train_x.columns[0]: 'id'})\n",
        "train_y = train_y.rename(columns={train_y.columns[0]: 'id'})\n",
        "\n",
        "data = pd.merge(train_x, train_y, on='id')\n",
        "test_ids = test_x['id'].values\n",
        "\n",
        "X_all = data.drop(['id', 'year'], axis=1).values.astype(np.float32)\n",
        "y_all = data['year'].values.astype(np.float32)\n",
        "X_test = test_x.drop('id', axis=1).values.astype(np.float32)"
      ],
      "metadata": {
        "id": "snjMzfIQE9W3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TabDataset(Dataset):\n",
        "    def __init__(self, features, targets=None):\n",
        "        self.features = features.astype(np.float32)\n",
        "        self.targets = targets.astype(np.float32) if targets is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.features[idx]).float()\n",
        "        if self.targets is not None:\n",
        "            y = torch.tensor(self.targets[idx]).float()\n",
        "            return x, y\n",
        "        return x\n",
        "\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_dim=512, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.input_ln = nn.LayerNorm(input_size)\n",
        "        self.fc_in = nn.Linear(input_size, hidden_dim)\n",
        "        self.act = nn.Mish()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Mish(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Mish(),\n",
        "            nn.Dropout(dropout*0.8)\n",
        "        )\n",
        "\n",
        "        self.fc_mid = nn.Linear(hidden_dim, hidden_dim//2)\n",
        "        self.mid_ln = nn.LayerNorm(hidden_dim//2)\n",
        "        self.dropout_final = nn.Dropout(dropout*0.5)\n",
        "        self.fc_out = nn.Linear(hidden_dim//2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_ln(x)\n",
        "        x = self.fc_in(x)\n",
        "        x = self.act(x)\n",
        "        x = x + self.block1(x)\n",
        "        x = x + self.block2(x)\n",
        "        x = self.fc_mid(x)\n",
        "        x = self.act(self.mid_ln(x))\n",
        "        x = self.dropout_final(x)\n",
        "        x = self.fc_out(x)\n",
        "        return x.view(-1)"
      ],
      "metadata": {
        "id": "nJreJpCaFELV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAYJNg9CanEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9628c628-3413-4aca-cb97-abc6cd32915c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5\n"
          ]
        }
      ],
      "source": [
        "# K-Fold кросс-валидация\n",
        "n_splits = 5  # Количество фолдов\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "test_preds_list = []  # Список для предсказаний с каждого фолда\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_all)):\n",
        "    print(f'Fold {fold + 1}/{n_splits}')\n",
        "\n",
        "    # Разделение данных\n",
        "    X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
        "    y_train, y_val = y_all[train_idx], y_all[val_idx]\n",
        "\n",
        "    # Масштабирование для каждого фолда\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # DataLoader\n",
        "    train_dataset = TabDataset(X_train, y_train)\n",
        "    val_dataset = TabDataset(X_val, y_val)\n",
        "    test_dataset = TabDataset(X_test_scaled)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
        "\n",
        "    # Инициализация модели для фолда\n",
        "    model = NN(input_size=X_train.shape[1]).to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Обучение\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Валидация\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb = xb.to(device)\n",
        "            preds = model(xb).cpu().numpy()\n",
        "            val_preds.append(preds)\n",
        "\n",
        "    val_preds = np.concatenate(val_preds).ravel()\n",
        "    val_mse = mean_squared_error(y_val, val_preds)\n",
        "    print(f\"Fold {fold + 1} Validation MSE: {val_mse:.6f}\")\n",
        "\n",
        "    # Предсказание на тесте\n",
        "    test_preds = []\n",
        "    with torch.no_grad():\n",
        "        for xb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            preds = model(xb).cpu().numpy()\n",
        "            test_preds.append(preds)\n",
        "\n",
        "    test_preds_list.append(np.concatenate(test_preds).ravel())\n",
        "\n",
        "# Усреднение предсказаний по фолдам\n",
        "test_preds_avg = np.mean(test_preds_list, axis=0)\n",
        "final_test_preds = np.rint(test_preds_avg).astype(int)\n",
        "\n",
        "# Создание submission файла\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_ids,\n",
        "    'year': final_test_preds\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission saved to submission.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}